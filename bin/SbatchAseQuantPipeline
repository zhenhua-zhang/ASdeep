#!/bin/bash
# Author:  Zhenhua Zhang
# E-mail:  zhenhua.zhang217@gmail.com
# Version: 0.2.0
# License: MIT

#
## ASE analysis pipeline by fastp, STAR, WASP, SAMtools, and aScan
#

# Rsync FASTQ files from calculon
# rsync -avzh umcg-zzhang@172.23.34.247:/groups/umcg-bios/prm02/rawdata/rnaseq/AD10W1ACXX-1-11_* .

#
## NOTE
#
# 1. For one sample with paired-end results about 2G in total, the intermediate
#    files could go up to 30G. Make sure have enough space on the devices.
# 2. To create the SNP HDF5 database, one has to split them into individual
#    files per chromosome if there are more than one chromosome.

#
## Working directory tree
#
# WORKDIR/
# ├── [genomeDir/]
# ├── [snpH5dbDir/]
# ├── optDir/
# │   └── FASTQID/
# │     ├── fastpOptDir/
# │     ├── starOptDir/
# │     ├── waspOptDir/
# │     ├── gatkOptDir/
# │     └── aseqOptDir/
# └── tmpDir
#   └── FASTQID/
#     ├── fastpTmpDir/
#     ├── starTmpDir/
#     ├── waspTmpDir/
#     ├── gatkTmpDir/
#       └── aseqTmpDir/

#
## WASP (v0.3.4) dependencies:
#
# Python 3.7.x
# Python packages:
#   - numpy==1.19.4
#   - pandas==1.2.3
#   - scipy==1.5.4
#   - pysam==0.16.0.1
#   - pyfaidx==0.5.9.5
#   - PyVCF==0.6.8

#
## Meta config {
#

# Load Easy-build modules
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
# } // Meta config

# ENVs {
export ASEPL_VERSION
ASEPL_VERSION="0.3.0"
ASEPL_FILENAME=$(basename "$0")
# } // ENVs

set -Ee -o pipefail

#
## Utilities {
#
# Error information, exit -1
erro() {
  echo -e "[E]: $1" >&2 && exit -1
}

# Warning information
warn() {
  echo -e "[W]: $*" >&2
}

# General information
info() {
  echo -e "[I]: $*"
}

echoVersion() {
  cat << EOF

$ASEPL_FILENAME, Version ${ASEPL_VERSION:=UNKNOWN}

EOF
}

# Echo help for the script
echo_help() {
  cat <<EOF

$ASEPL_FILENAME, Version ${ASEPL_VERSION:=UNKNOWN}

Help:
  -c, --conf   Required.
  The confituration file to supply values for the variables
  -h, --help   Optional.
  Print this help context and exit.
  -v, --version  Optional. Action: store_true
  Print version of current script and exit.

More information please contact Zhenhua Zhang <zhenhua.zhang217@gmail.com>

EOF
}
# } // Utilities

#
## CLI options {
#
opt=$(getopt -o "c:hv" -l "conf:,help,version" -- "$@")
eval set -- $opt
while true; do
  case $1 in
    -c|--conf) shift && config=$1 && break ;;
    -h|--help) echo_help && exit 0 ;;
    -v|--version) echoVersion && exit 0 ;;
    --) shift && break ;;
  esac
  shift
done

# Check the config file
[[ -f ${config:?-c/--conf is required} ]] \
  && source $config \
  || erro "Not found $config to read."
# } // CLI options

# This is an example config which can be specified by -c/--conf.
# #
# ## Config { # This is a template
# #
# cohortId=CODAM
# fastqId=AD10W1ACXX-1-11
# sampleId=188_2233
# 
# # Master directory
# projDir=/groups/umcg-bios/tmp01/users/umcg-zzhang/projects/wp_ase_dlp
# workDir=$projDir/outputs/aseQuan_v2
# 
# # SLURM logs
# logDir=$workDir/$cohortId/logDir 
# 
# # Temporary directory and files
# tmpDir=$workDir/$cohortId/tmpDir
# fastpTmpDir=$tmpDir/$fastqId/fastpTmpDir
# starTmpDir=$tmpDir/$fastqId/starTmpDir
# waspTmpDir=$tmpDir/$fastqId/waspTmpDir
# ascanTmpDir=$tmpDir/$fastqId/ascanTmpDir
## gatkTmpDir=$tmpDir/$fastqId/gatkTmpDir
## aseqTmpDir=$tmpDir/$fastqId/aseqTmpDir
# 
# # The final output results
# optDir=$workDir/$cohortId/optDir
# fastpOptDir=$optDir/$fastqId/fastpOptDir
# starOptDir=$optDir/$fastqId/starOptDir
# waspOptDir=$optDir/$fastqId/waspOptDir
# ascanOptDir=$optDir/$fastqId/ascanOptDir
## gatkOptDir=$optDir/$fastqId/gatkOptDir
## aseqOptDir=$optDir/$fastqId/aseqOptDir
# 
# # Genome sequence and gene structure
# genomeDir=$workDir/genomeDir
# genomeFastaFile=$projDir/inputs/GRCh37_reference/human_g1k_v37.fasta
# genomeAnnotationFile=$projDir/inputs/Ensembl_references/Homo_sapiens.GRCh37.75.gtf
# 
# # Genotypes (GT field is required)
# snpH5dbDir=$workDir/snpH5dbDir/$cohortId
# vcfFile=$projDir/outputs/phasing/all-$cohortId-singleAlt.vcf.gz
# chromInfoFile=$projDir/inputs/Ensembl_references/human_g1k_v37_chrom_info.txt
# 
# # FASTQ files
# fastqDir=$workDir/$cohortId/tmpDir
# fastqPrefix=
# fastqSuffix=.fq.gz
# 
# # Gene ids
# geneIdFile=$projDir/inputs/Ensembl_references/protein_coding_gene_id.txt
# 
# # Tools versions, Python virtual env
# pyEnv=~/Documents/projects/wp_ase_dlp/scripts/.env
# GCCVer=GCC/7.3.0-2.30
# GATKVer=GATK/4.1.4.1-Java-8-LTS
# HDF5Ver=HDF5/1.8.14-foss-2018b
# STARVer=STAR/2.6.1c-foss-2018b
# PythonVer=Python/3.7.4-GCCcore-7.3.0-bare
# BCFtoolsVer=BCFtools/1.11-GCCcore-7.3.0
# SAMtoolsVer=SAMtools/1.9-foss-2018b

# # Job dependency
# XXX: the colon after the afterok is maully added
afterok="--dependency=afterok"
# # } // Config

#
## Create the working directory tree
#
# TODO: Some directories are not necessary, remove them.
mkdir -p $logDir \
  $tmpDir/$fastqId/{fastp,star,wasp,ascan}TmpDir \
  $optDir/$fastqId/{fastp,star,wasp,ascan}OptDir

#
## STAR: Generate STAR genome index {
#
stepName=STARBuildGenomeIndex
if [ -d $genomeDir ]; then
  warn "Found $genomeDir, skip $stepName"
else
  [[ $genomeFastaFile"xxx" == "xxx" ]] \
    && erro "No genome index found, please give genome fasta file"

  [[ $genomeAnnotationFile"xxx" == "xxx" ]] \
    && erro "No genome index found, please give genome annotation file"

  mem=150G
  cpus=15
  time=0:59:00
  STARGenomeIndexJobId=$(sbatch \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$stepName \
    --output=$logDir/%j-$stepName.log \
    <<EOF | cut -d' ' -f4
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

mkdir -p $workDir/genomeDir

module load $STARVer
module list

STAR \
  --runMode genomeGenerate \
  --genomeDir $genomeDir \
  --genomeFastaFiles $genomeFastaFile \
  --sjdbGTFfile $genomeAnnotationFile \
  --runThreadN $cpus \
  --outFileNamePrefix $genomeDir/starGenomeIndex
EOF
)
info "$stepName was submitted: $STARGenomeIndexJobId ..."
fi
# } // STAR: Generate STAR genome index

#
## WASP: Generate SNP HDF5 database for WASP {
#
stepName=WASPCreateSnpHdf5Database
if [ -d $snpH5dbDir ]; then
  warn "Found $snpH5dbDir, skip $stepName"
else
  mem=5G
  cpus=1
  time=3:59:00
  WaspCreateSnpHdf5DatabaseJobId=$(sbatch \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$stepName \
    --output=$logDir/%j-%u-$stepName.log \
    <<EOF | cut -f4 -d ' '
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

mkdir -p $snpH5dbDir $tmpDir/snpH5dbTmpDir

# Split the VCF file by chromosome
module load $BCFtoolsVer
module list

## Index the VCF file, if the default one does not exist.
if [[ ! -e $vcfFile.bai ]]; then
  bcftools index --tbi --force --threads $cpus $vcfFile
fi

## Split
seq 1 22 | xargs \
  -n 1 \
  -I '{}' bcftools view -Oz -o \
    $tmpDir/snpH5dbTmpDir/$cohortId-chr{}.vcf.gz $vcfFile {}

# Generate SNP HDF5 database
module purge
module load $HDF5Ver
module list
~/tools/bin/snp2h5 \
  --chrom $chromInfoFile \
  --format vcf \
  --snp_tab $snpH5dbDir/snps_tab.h5 \
  --snp_index $snpH5dbDir/snps_index.h5 \
  --haplotype $snpH5dbDir/haplotype.h5 \
  $tmpDir/snpH5dbTmpDir/$cohortId-chr*.vcf.gz

# Clean up
rm -fr $tmpDir/snpH5dbTmpDir
EOF
)
info "$stepName was submitted: $WaspCreateSnpHdf5DatabaseJobId ..."
fi
# } // Generate SNP HDF5 database for WASP
# exit

#
## FASTP: Preprocessing fastq files. {
#
mem=2G
cpus=1
time=0:59:00
stepName=FastpPreproc
dependency=$afterok
if [[ $STARGenomeIndexJobId != '' ]]; then
  dependency=$dependency:$STARGenomeIndexJobId
fi

if [[ $WaspCreateSnpHdf5DatabaseJobId != '' ]]; then
  dependency=$dependency:$WaspCreateSnpHdf5DatabaseJobId
fi

if [[ $dependency == "--dependency=afterok" ]]; then dependency=""; fi
fastpPreprocJobId=$(sbatch $dependency \
  --mem=$mem \
  --time=$time \
  --cpus-per-task=$cpus \
  --job-name=$fastqId-$stepName \
  --output=$logDir/%j-$fastqId-$stepName.log \
  <<EOF | cut -d' ' -f4
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

module load $GCCVer # fastp was compiled using GCC v7.3.0
module list

# add a CLI option to fastq files
~/tools/bin/fastp \
  --in1 $fastqDir/$fastqPrefix$fastqId"_R1"$fastqSuffix \
  --in2 $fastqDir/$fastqPrefix$fastqId"_R2"$fastqSuffix \
  --out1 $fastpTmpDir/${fastqId}_paired_R1.fq.gz \
  --out2 $fastpTmpDir/${fastqId}_paired_R2.fq.gz \
  --unpaired1 $fastpTmpDir/${fastqId}_unpaired_R1.fq.gz \
  --unpaired2 $fastpTmpDir/${fastqId}_unpaired_R2.fq.gz  \
  --failed_out $fastpTmpDir/${fastqId}_failed.fq.gz \
  --html $fastpTmpDir/${fastqId}_report.html \
  --json $fastpTmpDir/${fastqId}_report.json \
  --thread $cpus \
  --overrepresentation_sampling 100 \
  --detect_adapter_for_pe \
  --cut_front \
  --cut_tail \
  --correction \
  --trim_poly_g \
  --trim_poly_x
EOF
)
info $stepName was submitted: $fastpPreprocJobId ...
# } // FASTP: Preprocessing fastq files.

#
## STAR: Mapping reads to genome {
#
mem=35G  # XXX: For human genome, it requires at least 40G memory.
cpus=6
time=0:59:00
stepName=STARMapping
dependency=$afterok:$fastpPreprocJobId
if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
STARMappingJobId=$(sbatch $dependency \
  --qos=priority \
  --mem=$mem \
  --time=$time \
  --cpus-per-task=$cpus \
  --job-name=$fastqId-$stepName \
  --output=$logDir/%j-$fastqId-$stepName.log \
  <<EOF | cut -d' ' -f4
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

# Mapping
module load $STARVer
module list

STAR --runMode alignReads \
  --genomeDir $genomeDir \
  --runThreadN $cpus \
  --readFilesIn $fastpTmpDir/${fastqId}_paired_R{1,2}.fq.gz \
  --readFilesCommand zcat \
  --outSAMtype BAM SortedByCoordinate \
  --twopassMode "Basic" \
  --outFileNamePrefix $starTmpDir/$fastqId.

# Index the output BAM file
module load $SAMtoolsVer
module list

# Add read group information
samtools addreplacerg \
  --threads $cpus \
  --output-fmt BAM \
  -r "ID:$fastqId\tSM:$fastqId\tPL:illumina" \
  -o $starTmpDir/$fastqId.newrg.bam \
  $starTmpDir/$fastqId".Aligned.sortedByCoord.out.bam"

mv -f $starTmpDir/$fastqId.newrg.bam $starTmpDir/$fastqId.bam

samtools index -@ $cpus $starTmpDir/$fastqId.bam
EOF
)
info "$stepName was submitted: $STARMappingJobId ..."
# } // Mapping reads to genome 

#
## WASP: Find intersected SNPs {
#
mem=15G
cpus=1
time=1:59:00
stepName=WASPRMBFindIntersectedSnps
dependency=$afterok:$STARMappingJobId
if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
WaspRMBFindIntersectedSnpsJobId=$(sbatch $dependency \
  --mem=$mem \
  --time=$time \
  --cpus-per-task=$cpus \
  --job-name=$fastqId-$stepName \
  --output=$logDir/%j-$fastqId-$stepName.log \
  <<EOF | cut -f4 -d' '
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

module load $HDF5Ver $PythonVer
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi

# Find intersecting SNPs
python ~/tools/WASP/mapping/find_intersecting_snps.py \
  --is_sorted \
  --is_paired_end \
  --output_dir $waspTmpDir \
  --snp_tab $snpH5dbDir/snps_tab.h5 \
  --snp_index $snpH5dbDir/snps_index.h5 \
  --haplotype $snpH5dbDir/haplotype.h5 \
  --samples $sampleId \
  $starTmpDir/$fastqId.bam

mv -f $waspTmpDir/$fastqId.keep.bam $waspTmpDir/$fastqId.keep.sam
mv -f $waspTmpDir/$fastqId.to.remap.bam $waspTmpDir/$fastqId.to.remap.sam

# Convert the SAM into BAM. Note: find_intersecting_snps.py output SAM not BAM
module purge
module load $SAMtoolsVer
module list

samtools view \
  -hbo $waspTmpDir/$fastqId.keep.bam \
  $waspTmpDir/$fastqId.keep.sam

samtools view \
  -hbo $waspTmpDir/$fastqId.to.remap.bam \
  $waspTmpDir/$fastqId.to.remap.sam

rm -f $waspTmpDir/$fastqId.keep.sam $waspTmpDir/$fastqId.to.remap.sam
EOF
)
info "$stepName was submitted: $WaspRMBFindIntersectedSnpsJobId ..."
# } // Find intersected SNPs

#
## STAR: Remapping by STAR for WASP {
#
mem=40G
cpus=1
time=0:59:00
stepName=WASPRMBRemapping
dependency=$afterok:$WaspRMBFindIntersectedSnpsJobId
if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
WaspRMBRemappingJobId=$(sbatch $dependency \
  --qos=priority \
  --mem=$mem \
  --time=$time \
  --cpus-per-task=$cpus \
  --job-name=$fastqId-$stepName \
  --output=$logDir/%j-$fastqId-$stepName.log \
  <<EOF | cut -f4 -d' '
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

module load $STARVer
module list

STAR \
  --runMode alignReads \
  --genomeDir $genomeDir \
  --runThreadN $cpus \
  --readFilesIn $waspTmpDir/$fastqId.remap.fq{1,2}.gz \
  --readFilesCommand zcat \
  --outSAMtype BAM SortedByCoordinate \
  --twopassMode "Basic" \
  --outFileNamePrefix $waspTmpDir/$fastqId.remap

# Create index files for BAMs
module load $SAMtoolsVer
module list

# FIXME: if you change the STAR options, the output suffix will change.
mv -f $waspTmpDir/$fastqId.remapAligned.sortedByCoord.out.bam \
  $waspTmpDir/$fastqId.remap.bam

samtools index -@ $cpus $waspTmpDir/$fastqId.remap.bam
EOF
)
info "$stepName was submitted: $WaspRMBRemappingJobId ..."
# } // STAR: Remapping by STAR for WASP

#
## WASP: RMB filtering and removing duplication {
#
mem=3G
cpus=1
time=1:29:00
stepName=WASPRMBFilterAndRmDup
dependency=$afterok:$WaspRMBRemappingJobId
if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
WaspRMBFilterAndRmDupJobId=$(sbatch $dependency \
  --mem=$mem \
  --time=$time \
  --cpus-per-task=$cpus \
  --job-name=$fastqId-$stepName \
  --output=$logDir/%j-$fastqId-$stepName.log \
    <<EOF | cut -f4 -d' '
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

# Filter remapped reads
module load $HDF5Ver $PythonVer
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
python ~/tools/WASP/mapping/filter_remapped_reads.py \
  $waspTmpDir/${fastqId}.to.remap.bam \
  $waspTmpDir/${fastqId}.remap.bam \
  $waspTmpDir/${fastqId}.remap.keep.bam

# Merge original and remapped BAM, then sort and index it
module purge
module load $SAMtoolsVer
module list

## Merge
samtools merge \
  -f \
  $waspTmpDir/${fastqId}.keep.merged.bam \
  $waspTmpDir/${fastqId}.remap.keep.bam \
  $waspTmpDir/${fastqId}.keep.bam

## Sort
samtools sort \
  -@ $cpus \
  -o $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
  $waspTmpDir/${fastqId}.keep.merged.bam

## Index
samtools index \
  -@ $cpus \
  $waspTmpDir/${fastqId}.keep.merged.sorted.bam

# # The allelic read counts by bam2h5.py of WASP are in messy, port to
# GTAK/ASEReadCounter instead.
# # Count allelic reads (with duplications)
# module purge
# module load $HDF5Ver $PythonVer
# module list
# 
# if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
# python ~/tools/WASP/CHT/bam2h5.py \
#   --chrom $chromInfoFile \
#   --individual $sampleId \
#   --snp_index $snpH5dbDir/snps_index.h5 \
#   --snp_tab $snpH5dbDir/snps_tab.h5 \
#   --haplotype $snpH5dbDir/haplotype.h5 \
#   --txt_counts $waspTmpDir/${fastqId}_dup_allAlleleReadCounts.txt \
#   --read_counts $waspTmpDir/${fastqId}_dup_allAlleleReadCounts.h5 \
#   --ref_as_counts $waspTmpDir/${fastqId}_dup_refAlleleReadCounts.h5 \
#   --alt_as_counts $waspTmpDir/${fastqId}_dup_altAlleleReadCounts.h5 \
#   --other_as_counts $waspTmpDir/${fastqId}_dup_otherAlleleReadCounts.h5 \
#   $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
#   2>&1 | grep -v WARNING


# Remove duplications
module purge
module load $HDF5Ver $PythonVer
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
python ~/tools/WASP/mapping/rmdup_pe.py \
  $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
  $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.bam

# Sort and index the non-duplicated bam file
module purge
module load $SAMtoolsVer
module list

## Sort
samtools sort \
  -@ $cpus \
  -o $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam \
  $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.bam

## Index
samtools index \
  -@ $cpus \
  $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam

# The allelic read counts by bam2h5.py of WASP are in messy, port to
# GTAK/ASEReadCounter instead.
# # Count allelic reads (without duplications)
# module purge
# module load $HDF5Ver $PythonVer
# module list
# 
# if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
# python ~/tools/WASP/CHT/bam2h5.py \
#   --chrom $chromInfoFile \
#   --individual $sampleId \
#   --snp_index $snpH5dbDir/snps_index.h5 \
#   --snp_tab $snpH5dbDir/snps_tab.h5 \
#   --haplotype $snpH5dbDir/haplotype.h5 \
#   --txt_counts $waspTmpDir/${fastqId}_nondup_allAlleleReadCounts.txt \
#   --read_counts $waspTmpDir/${fastqId}_nondup_allAlleleReadCounts.h5 \
#   --ref_as_counts $waspTmpDir/${fastqId}_nondup_refAlleleReadCounts.h5 \
#   --alt_as_counts $waspTmpDir/${fastqId}_nondup_altAlleleReadCounts.h5 \
#   --other_as_counts $waspTmpDir/${fastqId}_nondup_otherAlleleReadCounts.h5 \
#   $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam
#   2>&1 | grep -v WARNING
EOF
)
info "$stepName was submitted: $WaspRMBFilterAndRmDupJobId ..."
# } // WASP: RMB filtering and removing duplication

#
## aScan: Estimate transcript ASE {
#
mem=10G
cpus=1
time=6:59:00
stepName=aScanEstimateASE
dependency=$afterok:$WaspRMBFilterAndRmDupJobId
if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
aScanEstimateASEJobId=$(sbatch $dependency \
  --mem=$mem \
  --time=$time \
  --cpus-per-task=$cpus \
  --job-name=$fastqId-$stepName \
  --output=$logDir/%j-$fastqId-$stepName.log \
  <<EOF | cut -d' ' -f4
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

~/tools/bin/aScan

EOF
)
info "$stepName was submitted: $aScanEstimateASEJobId ..."
# } // aScan: Estimate transcript ASE

# #
# ## GATK: ASEReadCounter counts reads allelicly {
# #
# mem=10G
# cpus=1
# time=6:59:00
# stepName=GATKASEReadCounter
# dependency=$afterok:$WaspRMBFilterAndRmDupJobId
# if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
# GatkASEReadCounterJobId=$(sbatch $dependency \
#   --mem=$mem \
#   --time=$time \
#   --cpus-per-task=$cpus \
#   --job-name=$fastqId-$stepName \
#   --output=$logDir/%j-$fastqId-$stepName.log \
#   <<EOF | cut -d' ' -f4
# #!/bin/bash
# [[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
# set -Ee -o pipefail
# 
# module load $GATKVer
# module list
# 
# gatk ASEReadCounter \
#   --variant $vcfFile \
#   --output-format CSV \
#   --reference $genomeFastaFile \
#   --input $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
#   --output $gatkTmpDir/${fastqId}_dup_GATK_allelic_read_counts.csv \
#   2>&1 | grep -v WARN
# 
# gatk ASEReadCounter \
#   --variant $vcfFile \
#   --output-format CSV \
#   --reference $genomeFastaFile \
#   --input $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam \
#   --output $gatkTmpDir/${fastqId}_nondup_GATK_allelic_read_counts.csv \
#   2>&1 | grep -v WARN
# EOF
# )
# info "$stepName was submitted: $GatkASEReadCounterJobId ..."
# # } // GATK: ASEReadCounter counts reads allelicly

# #
# ## aseq estimates ASE effects {
# #
# mem=10G
# cpus=2
# time=5:59:00
# stepName=AseqASEEstimation
# dependency=$afterok:$GatkASEReadCounterJobId
# if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
# AseqASEEstimationJobId=$(sbatch $dependency \
#   --mem=$mem \
#   --time=$time \
#   --cpus-per-task=$cpus \
#   --job-name=$fastqId-$stepName \
#   --output=$logDir/%j-$fastqId-$stepName.log \
#   <<EOF | cut -d' ' -f4
# #!/bin/bash
# [[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
# set -Ee -o pipefail
# 
# module load $PythonVer
# if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
# 
# python $projDir/scripts/asedlp quantify \
#   -v $vcfFile \
#   -i $sampleId \
#   -G $geneIdFile \
#   -s $genomeFastaFile \
#   -f $genomeAnnotationFile \
#   -r $gatkTmpDir/${fastqId}.gatkAlleleReadCounts.csv \
#   -T $aseqTmpDir/$fastqId-train-set.fa.gz \
#   -R $aseqTmpDir/$fastqId-train-report.txt &
# 
# python $projDir/scripts/asedlp quantify \
#   -v $vcfFile \
#   -i $sampleId \
#   -G $geneIdFile \
#   -s $genomeFastaFile \
#   -f $genomeAnnotationFile \
#   -r $gatkTmpDir/${fastqId}.nondup.gatkAlleleReadCounts.csv \
#   -T $aseqTmpDir/$fastqId-nondup-train-set.fa.gz \
#   -R $aseqTmpDir/$fastqId-nondup-train-report.txt &
# 
# wait
# EOF
# )
# info "$stepName was submitted: $AseqASEEstimationJobId ..."
# # } // aseq estimates ASE effects

#
## Collect output and clean up {
#
mem=500M
cpus=1
time=0:5:00
stepName=CollectOutputAndCleanup
dependency=$afterok:$AseqASEEstimationJobId
if [[ $dependency == "--dependency=afterok:" ]]; then dependency=""; fi
CollectOutputAndCleanupJobId=$(sbatch $dependency \
  --mem=$mem \
  --time=$time \
  --cpus-per-task=$cpus \
  --job-name=$fastqId-$stepName \
  --output=$logDir/%j-$fastqId-$stepName.log \
  <<EOF | cut -d' ' -f4
#!/bin/bash
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
set -Ee -o pipefail

# Copy fastp reports to output directory.
cp -fr $fastpTmpDir/$fastqId*.{html,json} $fastpOptDir

# Copy GATK/ASEReadCounter outputs to output directory
cp -fr $gatkTmpDir/$fastqId*.csv $gatkOptDir

# Copy asedlp/quantify outputs to output directory
cp -fr $aseqTmpDir/$fastqId*.{fa.gz,txt} $aseqOptDir

# Remove fastq files and temporary directory (tmpDir/$fastqId)
rm -fr $tmpDir/$fastqId
rm -fr $tmpDir/$fastqId"_"R{1,2}.fq.gz
EOF
)
info "$stepName was submitted. Job id: $CollectOutputAndCleanupJobId ..."
# } // Collect output and clean up

# vim: set ai nowrap nospell ft=sh:
