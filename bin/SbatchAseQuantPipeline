#!/bin/bash
# Author:  Zhenhua Zhang
# E-mail:  zhenhua.zhang217@gmail.com
# Version: 0.2.0
# License: MIT

#
## ASE analysis pipeline by fastp, STAR, WASP, SAMtools, and GATK/ASEReadCounter
#

#
## XXX: Caution!
#
# 1. For one sample with paired-end results about 2G in total, the intermediate files could go up to
#    30G. Please make sure you have enough space on the devices.
# 2. To create the SNP HDF5 database, one has to split them into individual files per chromosome,
#    if there are more than one chromosome.

#
## Working directory tree
#
# WORKDIR/
# ├── [genomeDir/]
# ├── [snpH5dbDir/]
# ├── optDir/
# │   └── FASTQID/
# │       ├── fastpOptDir/
# │       ├── starOptDir/
# │       ├── waspOptDir/
# │       ├── gatkOptDir/
# │       └── aseqOptDir/
# └── tmpDir
#     └── FASTQID/
#         ├── fastpTmpDir/
#         ├── starTmpDir/
#         ├── waspTmpDir/
#         ├── gatkTmpDir/
#         └── aseqTmpDir/

#
## WASP (v0.3.4) dependencies:
#
# Python 3.6.3
# Python packages:
#   - numexpr 2.7.1
#   - numpy 1.19.4
#   - pysam 0.16.0.1
#   - scipy 1.5.4
#   - tables 3.6.1

#
## Meta config {
#
set -Ee -o pipefail

# Load some utility functions
WaspPipelineUtils_file=$(dirname $(realpath $0))/WaspPipelineUtils.sh
if [[ -f $WaspPipelineUtils_file ]]; then
    source $WaspPipelineUtils_file
else
    echo Not found WaspPipelineUtils.sh && exit -1
fi

# Load Easy-build modules
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc
# } // Meta config


#
## CLI options { TODO: plan to use a config file
#
opt=$(getopt -o "c:hv" -l "conf:,help,version" -- "$@")
eval set -- $opt
while true; do
    case $1 in
        -c|--conf) shift && config=$1 && break ;;
        -h|--help) echoHelp && exit 0 ;;
        -v|--version) echoVersion && exit 0 ;;
        --) shift && break ;;
    esac
    shift
done

# Check the config file
# [[ -f ${config:?-c/--conf is required} ]] \
#     && source $config || echoErro "Not found $config to read"
# } // CLI options

#
## Config {
#
projDir=/groups/umcg-bios/tmp01/users/umcg-zzhang/projects/wp_ase_dlp
workDir=$projDir/temps/runTest

cohortId=CODAM
fastqId=AD10W1ACXX-1-11

# SLURM logs
logDir=$workDir/$cohortId/logDir 

# Temporary directory and files
tmpDir=$workDir/$cohortId/tmpDir
fastpTmpDir=$tmpDir/$fastqId/fastpTmpDir
starTmpDir=$tmpDir/$fastqId/starTmpDir
waspTmpDir=$tmpDir/$fastqId/waspTmpDir
gatkTmpDir=$tmpDir/$fastqId/gatkTmpDir
aseqTmpDir=$tmpDir/$fastqId/aseqTmpDir

# The final output results
optDir=$workDir/$cohortId/optDir
fastpOptDir=$OptDir/$fastqId/fastpOptDir
starOptDir=$OptDir/$fastqId/starOptDir
waspOptDir=$OptDir/$fastqId/waspOptDir
gatkOptDir=$OptDir/$fastqId/gatkOptDir
aseqOptDir=$OptDir/$fastqId/aseqOptDir

# Genome sequence and gene structure
genomeDir=$workDir/genomeDir
genomeFastaFile=$projDir/inputs/GRCh37_reference/human_g1k_v37.fasta
genomeAnnotationFile=$projDir/inputs/Ensembl_references/Homo_sapiens.GRCh37.75.gtf

# Genotypes (GT field is required)
snpH5dbDir=$workDir/snpH5dbDir/$cohortId
vcfFile=$projDir/outputs/phasing/all-$cohortId-singleAlt.vcf.gz
chromInfoFile=$projDir/inputs/Ensembl_references/human_g1k_v37_chrom_info.txt

# FASTQ files
fastqDir=$workDir/$cohortId/tmpDir
fastqPrefix=
fastqSuffix=.fq.gz

# ID pairs: FASTQ id vs genotype id
sampleIdFile=$projDir/inputs/idmapping/BIOS-genotype-rnaseq-ids_with-LL_usable_20210105.txt
sampleId=$(grep -w $fastqId $sampleIdFile | cut -f2)

# Python virtual environment
pyEnv=~/Documents/projects/wp_ase_dlp/scripts/.env

# Job dependency
afterok="--dependency=afterok"  # XXX: the colon after the afterok is maully added
# } // Config

#
## Create the working directory
#
mkdir -p $logDir \
    $tmpDir/$fastqId/{fastp,star,wasp,gatk,aseq}TmpDir \
    $optDir/$fastqId/{fastp,star,wasp,gatk,aseq}OptDir \

#
## STAR: Generate STAR genome index {
#
stepName=STARBuildGenomeIndex
if [ -d $genomeDir ]; then
    echoWarn "Found $genomeDir, skip $stepName"
else
    [[ $genomeFastaFile"xxx" == "xxx" ]] \
        && echoErro "No genome index found, please give genome fasta file"

    [[ $genomeAnnotationFile"xxx" == "xxx" ]] \
        && echoErro "No genome index found, please give genome annotation file"

    mem=150G
    cpus=15
    time=0:59:00
    STARGenomeIndexJobId=$(echo sbatch \
        --mem=$mem \
        --time=$time \
        --cpus-per-task=$cpus \
        --job-name=$stepName \
        --output=$workDir/logDir/%j-$stepName.log \
        <<EOF | cut -d' ' -f4
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

mkdir -p $workDir/genomeDir

module load STAR/2.6.1c-foss-2018b
module list

STAR \
    --runMode genomeGenerate \
    --genomeDir $genomeDir \
    --genomeFastaFiles $genomeFastaFile \
    --sjdbGTFfile $genomeAnnotationFile \
    --runThreadN $cpus \
    --outFileNamePrefix $genomeDir/starGenomeIndex
EOF
)
echoInfo "$stepName was submitted: $STARGenomeIndexJobId ..."
fi
# } // STAR: Generate STAR genome index

#
## WASP: Generate SNP HDF5 database for WASP {
#
stepName=WaspCreateSnpHdf5Database
if [ -d $snpH5dbDir ]; then
    echoWarn "Found $snpH5dbDir, skip $stepName"
else
    mem=5G
    cpus=1
    time=0:59:00
    WaspCreateSnpHdf5DatabaseJobId=$(sbatch \
        --mem=$mem \
        --time=$time \
        --cpus-per-task=$cpus \
        --job-name=$stepName \
        --output=$logDir/%j-%u-$stepName.log \
        <<EOF | cut -f4 -d ' '
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

mkdir -p $snpH5dbDir
mkdir -p $tmpDir/snpH5dbTmpDir

# Split the VCF file by chromosome
module load BCFtools
module list

## Index the VCF file, if the default one does not exist. Make SURE you have write permission.
if [[ ! -e $vcfFile.bai ]]; then
    bcftools index \
        --tbi \
        --force \
        --threads $cpus \
        $vcfFile
fi

## Split
for chr in {1..22}; do
    bcftools view \
        -O z
        -o $tmpDir/snpH5dbTmpDir/$cohortId-chr${chr}.vcf.gz \
        $vcfFile ${chr}
done

# Generate SNP HDF5 database
module load HDF5/1.8.14-foss-2018b
module list
~/tools/bin/snp2h5 \
    --chrom $chromInfoFile \
    --format vcf \
    --snp_tab $snpH5dbDir/snps_tab.h5 \
    --snp_index $snpH5dbDir/snps_index.h5 \
    --haplotype $snpH5dbDir/haplotype.h5 \
    $tmpDir/snpH5dbTmpDir/$cohortId-chr*.vcf.gz

# Clean up
rm -fr $tmpDir/snpH5dbTmpDir
EOF
)
echoInfo "$stepName was submitted: $WaspCreateSnpHdf5DatabaseJobId ..."
fi
# } // Generate SNP HDF5 database for WASP

#
## FASTP: Preprocessing fastq files. {
#
mem=5G
cpus=1
time=0:19:00
stepName=FastpPreproc
dependency=$afterok:$STARMappingJobId:$WaspCreateSnpHdf5DatabaseJobId
if [[ $dependency == "--afterok::" ]]; then dependency=""; fi
fastpPreprocJobId=$(sbatch $dependency \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$fastqId-$stepName \
    --output=$logDir/%j-$fastqId-$stepName.log \
    <<EOF | cut -d' ' -f4
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

module load GCC/7.3.0-2.30 # fastp was compiled using GCC v7.3.0
module list

# add a CLI option to fastq files
~/tools/bin/fastp \
    --in1 $fastqDir/$fastqPrefix$fastqId"_R1"$fastqSuffix \
    --in2 $fastqDir/$fastqPrefix$fastqId"_R2"$fastqSuffix \
    --out1 $fastpTmpDir/${fastqId}_paired_R1.fq.gz \
    --out2 $fastpTmpDir/${fastqId}_paired_R2.fq.gz \
    --unpaired1 $fastpTmpDir/${fastqId}_unpaired_R1.fq.gz \
    --unpaired2 $fastpTmpDir/${fastqId}_unpaired_R2.fq.gz  \
    --failed_out $fastpTmpDir/${fastqId}_failed.fq.gz \
    --html $fastpTmpDir/${fastqId}_report.html \
    --json $fastpTmpDir/${fastqId}_report.json \
    --thread $cpus \
    --overrepresentation_sampling 100 \
    --detect_adapter_for_pe \
    --cut_front \
    --cut_tail \
    --correction \
    --trim_poly_g \
    --trim_poly_x
EOF
)
echoInfo $stepName was submitted: $fastpPreprocJobId ...
# } // FASTP: Preprocessing fastq files.

#
## STAR: Mapping reads to genome {
#
mem=50G  # XXX: For human genome, it requires at least 40G memory.
cpus=1
time=0:45:00
stepName=STARMapping
dependency=$afterok:$fastpPreprocJobId
STARMappingJobId=$(sbatch $dependency \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$fastqId-$stepName \
    --output=$workDir/logDir/%j-$fastqId-$stepName.log \
    <<EOF | cut -d' ' -f4
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

# Mapping
module load STAR/2.6.1c-foss-2018b
module list

STAR --runMode alignReads \
    --genomeDir $genomeDir \
    --runThreadN $cpus \
    --readFilesIn $fastpTmpDir/${fastqId}_paired_R{1,2}.fq.gz \
    --readFilesCommand zcat \
    --outSAMtype BAM SortedByCoordinate \
    --twopassMode "Basic" \
    --outFileNamePrefix $starTmpDir/$fastqId.

# Index the output BAM file
module load SAMtools/1.9-foss-2018b
module list

# Add read group information
samtools addreplacerg \
    --threads $cpus \
    --output-fmt BAM \
    -r "ID:$fastqId\tSM:$fastqId\tPL:illumina" \
    -o $starTmpDir/$fastqId.newrg.bam \
    $starTmpDir/$fastqId".Aligned.sortedByCoord.out.bam"

mv -f $starTmpDir/$fastqId.newrg.bam $starTmpDir/$fastqId".Aligned.sortedByCoord.out.bam"

samtools index -@ $cpus $starTmpDir/$fastqId*.bam
EOF
)
echoInfo "$stepName was submitted: $STARMappingJobId ..."
# } // Mapping reads to genome 

#
## WASP: Find intersected SNPs {
#
mem=8G
cpus=1
time=0:49:00
stepName=WaspRMBFindIntersectedSnps
dependency=$afterok:$STARMappingJobId
WaspRMBFindIntersectedSnpsJobId=$(sbatch $dependency \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$fastqId-$stepName \
    --output=$logDir/%j-$fastqId-$stepName.log \
    <<EOF | cut -f4 -d' '
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

module load HDF5/1.8.14-foss-2018b Python/3.7.4-GCCcore-7.3.0-bare
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi

# Find intersecting SNPs
python ~/tools/WASP/mapping/find_intersecting_snps.py \
    --is_sorted \
    --is_paired_end \
    --output_dir $waspTmpDir \
    --snp_tab $snpH5dbDir/snps_tab.h5 \
    --snp_index $snpH5dbDir/snps_index.h5 \
    --haplotype $snpH5dbDir/haplotype.h5 \
    --samples $sampleId \
    $starTmpDir/$fastqId".Aligned.sortedByCoord.out.bam"

mv -f $waspTmpDir/$fastqId".Aligned.sortedByCoord.out.keep.bam" $waspTmpDir/$fastqId".keep.bam"
mv -f $waspTmpDir/$fastqId".Aligned.sortedByCoord.out.to.remap.bam" $waspTmpDir/$fastqId".to.remap.bam"
mv -f $waspTmpDir/$fastqId".Aligned.sortedByCoord.out.remap.fq1.gz" $waspTmpDir/$fastqId".remap.fq1.gz" 
mv -f $waspTmpDir/$fastqId".Aligned.sortedByCoord.out.remap.fq2.gz" $waspTmpDir/$fastqId".remap.fq2.gz" 
mv -f $waspTmpDir/$fastqId".Aligned.sortedByCoord.out.remap.single.fq.gz" $waspTmpDir/$fastqId".remap.single.fq.gz" 
EOF
)
echoInfo "$stepName was submitted: $WaspRMBFindIntersectedSnpsJobId ..."
# } // Find intersected SNPs

#
## STAR: Remapping by STAR for WASP {
#
mem=50G
cpus=5
time=0:29:00
stepName=WaspRMBRemapping
dependency=$afterok:$WaspRMBFindIntersectedSnpsJobId
WaspRMBRemappingJobId=$(sbatch $dependency \
    --time=$time --mem=$mem --cpus-per-task=$cpus \
    --output=$workDir/logDir/%j-$fastqId-$stepName.log \
    --job-name=$fastqId-$stepName \
    <<EOF | cut -f4 -d' '
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

module load STAR/2.6.1c-foss-2018b
module list

STAR \
    --runMode alignReads \
    --genomeDir $genomeDir \
    --runThreadN $cpus \
    --readFilesIn $waspTmpDir/$fastqId.remap.fq{1,2}.gz \
    --readFilesCommand zcat \
    --outSAMtype BAM SortedByCoordinate \
    --twopassMode "Basic" \
    --outFileNamePrefix $waspTmpDir/$fastqId.remap

# Create index files for BAMs
module load SAMtools/1.9-foss-2018b
module list

# FIXME: if you change the STAR options, the output suffix will change.
mv -f $waspTmpDir/$fastqId.remapAligned.sortedByCoord.out.bam $waspTmpDir/$fastqId.remap.bam

samtools index -@ $cpus $waspTmpDir/$fastqId.remap.bam
EOF
)
echoInfo "$stepName was submitted: $WaspRMBRemappingJobId ..."
# } // STAR: Remapping by STAR for WASP

#
## WASP: RMB filtering and removing duplication {
#
mem=10G
cpus=1
time=0:59:00
stepName=WaspRMBFilterAndRmDup
dependency=$afterok:$WaspRMBRemappingJobId
WaspRMBFilterAndRmDupJobId=$(sbatch $dependency \
    --mem=$mem 
    --time=$time 
    --cpus-per-task=$cpus \
        --job-name=$fastqId-$stepName \
        --output=$workDir/logDir/%j-$fastqId-$stepName.log \
        <<EOF | cut -f4 -d' '
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

# Filter remapped reads
module load HDF5/1.8.14-foss-2018b Python/3.7.4-GCCcore-7.3.0-bare
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
python ~/tools/WASP/mapping/filter_remapped_reads.py \
    $waspTmpDir/${fastqId}.to.remap.bam \
    $waspTmpDir/${fastqId}.remap.bam \
    $waspTmpDir/${fastqId}.remap.keep.bam

# Merge original and remapped BAM, then sort and index it
module purge
module load SAMtools/1.9-foss-2018b
module list

## Merge
samtools merge \
    -f \
    $waspTmpDir/${fastqId}.keep.merged.bam \
    $waspTmpDir/${fastqId}.remap.keep.bam \
    $waspTmpDir/${fastqId}.keep.bam

## Sort
samtools sort \
    -@ $cpus \
    -o $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
    $waspTmpDir/${fastqId}.keep.merged.bam

## Index
samtools index \
    -@ $cpus \
    $waspTmpDir/${fastqId}.keep.merged.sorted.bam

# Count allelic reads (with duplications)
module purge
module load HDF5/1.8.14-foss-2018b Python/3.7.4-GCCcore-7.3.0-bare
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
python ~/tools/WASP/CHT/bam2h5.py \
    --chrom $chromInfoFile \
    --individual $sampleId \
    --snp_index $snpH5dbDir/snps_index.h5 \
    --snp_tab $snpH5dbDir/snps_tab.h5 \
    --haplotype $snpH5dbDir/haplotype.h5 \
    --txt_counts $waspTmpDir/${fastqId}.wasp.allAlleleReadCounts.txt \
    --read_counts $waspTmpDir/${fastqId}.wasp.allAlleleReadCounts.h5 \
    --ref_as_counts $waspTmpDir/${fastqId}.wasp.refAlleleReadCounts.h5 \
    --alt_as_counts $waspTmpDir/${fastqId}.wasp.altAlleleReadCounts.h5 \
    --other_as_counts $waspTmpDir/${fastqId}.wasp.otherAlleleReadCounts.h5 \
    $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
    2>1 | grep -v WARNING


# Remove duplications
module purge
module load HDF5/1.8.14-foss-2018b Python/3.7.4-GCCcore-7.3.0-bare
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
python ~/tools/WASP/mapping/rmdup_pe.py \
    $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
    $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.bam

# Sort and index the non-duplicated bam file
module purge
module load SAMtools/1.9-foss-2018b
module list

## Sort
samtools sort \
    -@ $cpus \
    -o $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam \
    $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.bam

## Index
samtools index \
    -@ $cpus \
    $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam


# Count allelic reads (without duplications)
module purge
module load HDF5/1.8.14-foss-2018b Python/3.7.4-GCCcore-7.3.0-bare
module list

if [ $pyEnv"xxx" != "xxx" ]; then source $pyEnv/bin/activate; fi
python ~/tools/WASP/CHT/bam2h5.py \
    --chrom $chromInfoFile \
    --individual $sampleId \
    --snp_index $snpH5dbDir/snps_index.h5 \
    --snp_tab $snpH5dbDir/snps_tab.h5 \
    --haplotype $snpH5dbDir/haplotype.h5 \
    --txt_counts $waspTmpDir/${fastqId}.nondup.wasp.allAlleleReadCounts.txt \
    --read_counts $waspTmpDir/${fastqId}.nondup.wasp.allAlleleReadCounts.h5 \
    --ref_as_counts $waspTmpDir/${fastqId}.nondup.wasp.refAlleleReadCounts.h5 \
    --alt_as_counts $waspTmpDir/${fastqId}.nondup.wasp.altAlleleReadCounts.h5 \
    --other_as_counts $waspTmpDir/${fastqId}.nondup.wasp.otherAlleleReadCounts.h5 \
    $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam
    2>1 | grep -v WARNING
EOF
)

echoInfo "$stepName was submitted: $WaspRMBFilterAndRmDupJobId ..."
# } // WASP: RMB filtering and removing duplication

#
## GATK: ASEReadCounter counts reads allelicly {
#
mem=25G
cpus=1
time=0:29:00
stepName=GatkASEReaderCounter
dependency=$afterok:$WaspRMBFilterAndRmDupJobId
GatkASEReaderCounterJobId=$(sbatch $dependency \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$fastqId-$stepName \
    --output=$workDir/logDir/%j-$fastqId-$stepName.log \
    <<EOF | cut -d' ' -f4
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

module load GATK
module list

gatk ASEReadCounter \
    --variant $vcfFile \
    --output-format CSV \
    --reference $genomeFastaFile \
    --input $waspTmpDir/${fastqId}.keep.merged.sorted.bam \
    --output $gatkTmpDir/$fastqId.gatkAlleleReadCounts.csv \
    2>1 | grep -v WARN

gatk ASEReadCounter \
    --variant $vcfFile \
    --output-format CSV \
    --reference $genomeFastaFile \
    --input $waspTmpDir/${fastqId}.keep.merged.sorted.rmdup.sorted.bam \
    --output $gatkTmpDir/$fastqId.nondup.gatkAlleleReadCounts.csv \
    2>1 | grep -v WARN
EOF
)
# } // GATK: ASEReadCounter counts reads allelicly

#
## asequan estimates ASE effects {
#
mem=1G
cpus=1
time=0:9:00
stepName=AsequanASEEstimation
dependency=$afterok:$WaspRMBFilterAndRmDupJobId:$GatkASEReaderCounterJobId
AsequanASEEstimationJobId=$(sbatch $dependency \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$fastqId-$stepName \
    --output=$workDir/logDir/%j-$fastqId-$stepName.log \
    <<EOF | cut -d' ' -f4
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

EOF
# } // asequan estimates ASE effects

#
## Collect output and clean up {
#
mem=1G
cpus=1
time=0:10:00
stepName=CollectOutputAndCleanup
dependency=$afterok:$GatkASEReaderCounterJobId:
CollectOutputAndCleanupJobId=$(sbatch $dependency \
    --mem=$mem \
    --time=$time \
    --cpus-per-task=$cpus \
    --job-name=$fastqId-$stepName \
    --output=$workDir/logDir/%j-$fastqId-$stepName.log \
    <<EOF | cut -d' ' -f4
#!/bin/bash
set -Ee -o pipefail
[[ -f /apps/modules/modules.bashrc ]] && source /apps/modules/modules.bashrc

# Copy fastp reports to output directory.
cp -fr $fastpTmpDir/*.{html,json} $fastpOptDir

# Copy WASP ASE read counts (including w/o duplications) to output directory.
cp -fr $waspTmpDir/$fastqId.*.txt $waspOptDir

# Copy GATK/ASEReadCounter outputs to
cp -fr $gatkTmpDir/$fastqId.*.{csv,txt} $gatkOptDir

# Remove fastq files and temporary directory (tmpDir/$fastqId)
rm -fr $tmpDir/$fastqId
rm -fr $tmpDir/$fastqId"_"R{1,2}.fq.gz
EOF
)

echoInfo "$stepName was submitted. Job id: $CollectOutputAndCleanupJobId ..."
# } // Collect output and clean up

# vim: set ai nowrap nospell ft=sh:
